{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Project of Data Science course FINAL.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb","_cell_guid":"d632b08c-d252-4238-b496-e2c6edebec4b","id":"_1MstEmCxIuI"},"source":["## Imports\n"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"3rOgi4iIxIuM"},"source":["import pandas as pd\n","import sklearn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import warnings\n","import seaborn as sns\n","import gc\n","import lightgbm as lgb\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, power_transform, PolynomialFeatures\n","from sklearn.impute import SimpleImputer\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n","from tensorflow import keras\n","from keras import Sequential\n","from keras.layers import Dense,Dropout\n","from keras.callbacks import EarlyStopping\n","from keras import metrics\n","from contextlib import contextmanager\n","from lightgbm import LGBMClassifier\n","from sklearn.svm import SVC\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from xgboost import XGBClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSWssrrh71qM"},"source":["# Definizione funzioni"]},{"cell_type":"code","metadata":{"_uuid":"ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355","_cell_guid":"70641d4d-1075-4837-8972-e58d70d8f242","trusted":true,"id":"NZ3KEJ5mxIxV"},"source":["'''\n","Method that apply the label encoder to the features\n","of the dataset passed in input.\n","The feature inside 'black_list' are not considered for the\n","label encoding.\n","@param df : the dataset input\n","@return the dataset with label encoding features\n","'''\n","def use_label_encoder(df):\n","    black_list = ['CODE_GENDER']\n","    le = LabelEncoder()\n","    le_count = 0\n","    converted_columns = []\n","    for col in df:\n","        if df[col].dtype == 'object' and col not in black_list:\n","            if len(list(df[col].unique())) <= 2:\n","                le.fit(df[col])\n","                df[col] = le.transform(df[col])\n","                le_count += 1\n","                converted_columns.append(col)\n","    return df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mSuCeVRYe49","trusted":true},"source":["'''\n","Method that return a list with all the feature names that contains\n","with days greater than 36500.\n","@param df : the dataset input\n","@return a list with features names\n","'''\n","def get_features_anomalies_on_days(training_data):\n","  result = []\n","  for x in training_data.columns:\n","    # if columns starts with DAYS and value >= 100 (years) then return a list with column's name\n","    cond = training_data[training_data[x] >= 36500].count()[1] > 0\n","    if x.startswith('DAYS') and cond:\n","      result.append(x)\n","  return result\n","\n","'''\n","Method that correct inplace the anomalies days of\n","the dataset passed in input \n","@param df : the dataset input\n","@param list_of_features : list with anomalies days features\n","'''\n","# if value equals 365243 then replace 365243 with nan and add a column with true \n","def correct_anomalies_on_days(data,list_of_features):\n","  for f in list_of_features:\n","    data[f+\"_ANOM\"] = data[f] >= 365243\n","    data[f].replace({365243: np.nan}, inplace = True)\n","\n","\n","'''\n","Method that applies the absolute value to the features\n","that contains days in negative of the dataset passed in\n","input\n","@param df : the dataset input\n","@return the dataset modified\n","'''\n","# if column starts with DAYS and value is negative then apply absolute function\n","def correct_negative_days(data):\n","  for x in data.columns:\n","    if x.startswith('DAYS') and data[data[x] < 0].count()[1] > 0:\n","      data[x] = abs(data[x])\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU1elhfGHNYv","trusted":true},"source":["'''\n","Method that reads a dataset and does some operations\n","of cleaning\n","@param file_path : the file.csv of the dataset to read\n","@return a dataset as dataframe \n","'''\n","def read_csv_and_clean(file_path, out_path = '/content/'):\n","  print(\"reading: \" + file_path)\n","  # read csv\n","  res = pd.read_csv(file_path)\n","\n","  # replace XNA and XAP with NaN\n","  res.replace(to_replace =['XNA', 'XAP'], value = np.nan, inplace = True)\n","\n","  # transform categorical features\n","  res = use_label_encoder(res)\n","  res = pd.get_dummies(res)\n","\n","  # correct anomalies on days\n","  correct_anomalies_on_days(res, get_features_anomalies_on_days(res))\n","  res = correct_negative_days(res)\n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJ1U_WC7neLo","trusted":true},"source":["'''\n","Method that merges the dataset passed in input to the\n","application.csv dataset through the SK_ID_CURR\n","@param df_application : the application.csv dataset \n","@param df_to_merge : the dataset to merge\n","@param suffix : the suffix to join to new merged features names\n","@param prefix : the prefix to join to new merged features names\n","@return the dataset merged\n","'''\n","def merge_file_to_application(df_application,df_to_merge,suffix,prefix):\n","  #count how many samples are correlated to the primary key and merge them\n","  grp = df_to_merge[['SK_ID_CURR','SK_ID_'+suffix]].groupby(by=['SK_ID_CURR'])['SK_ID_'+suffix].count().reset_index().rename(columns={'SK_ID_'+suffix: prefix+'_COUNT'})\n","  df_application = df_application.merge(grp, on =['SK_ID_CURR'], how = 'left')\n","  #fill NaN with 0\n","  df_application[prefix+'_COUNT'] = df_application[prefix+'_COUNT'].fillna(0)\n","\n","  #merge the numerical features using the mean\n","  grp = df_to_merge.drop('SK_ID_'+suffix, axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n","  grp.columns = change_column_name(grp.columns,prefix)\n","  df_application = df_application.merge(grp, on =['SK_ID_CURR'], how = 'left')\n","  df_application.update(df_application[grp.columns].fillna(0))\n","  return df_application\n","\n","'''\n","Method that change the column names to the columns list passed in input\n","@param columns : list of columns to change\n","@param prefix : the prefix to join to the columns names passed in input\n","@return a list of columns changed\n","'''\n","def change_column_name(columns, prefix):\n","    result = []\n","    for c in columns:\n","      if c != 'SK_ID_CURR' and c != 'SK_ID_BUREAU':\n","        result.append(prefix +'_'+ c)\n","      else:\n","        result.append(c)\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"op0fuVR9I6PF","trusted":true},"source":["'''\n","Method that align the shape of the training and test data\n","@param training_data : the dataset of train \n","@param test_data : the dataset of test\n","@return the both datasets aligned\n","'''\n","def align_train_test_sets(training_data,test_data):\n","    train_labels = training_data['TARGET']\n","\n","    prev_cols_train = training_data.columns\n","    prev_cols_test = test_data.columns\n","    training_data, test_data = training_data.align(test_data, join = 'inner', axis = 1)\n","\n","    training_data['TARGET'] = train_labels\n","\n","    return training_data,test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwiQ5CENV-FY"},"source":["'''\n","Method that for each list passed in input, reduce the features inside in 1 pca component and \n","replace them whit this latter.\n","@param train_data : the train data\n","@param test_data : the test data\n","@param list_of_list_features : list that contains list of features to reduce\n","@return the train data and test data reduced\n","'''\n","def reduce_with_PCA(train_data,test_data,list_of_list_features):\n","\n","  train = train_data.copy()\n","  test = test_data.copy()\n","\n","  #define imputer and PCA\n","  imputer = SimpleImputer(strategy = 'median')\n","  pca = PCA(n_components=1)\n","\n","  for i,l in enumerate(list_of_list_features):\n","    print(\"PCA REDUCTION NÂ°\",i)\n","    print(\"shape train before reduction -->\",train.shape)\n","    print(\"shape test before reduction -->\",test.shape)\n","    #remove from train and test the features to reduce\n","    features_data_train = train[l]\n","    features_data_test = test[l]\n","    \n","    #impute data\n","    features_data_train = imputer.fit_transform(features_data_train)\n","    features_data_test = imputer.transform(features_data_test)\n","\n","    #reduce with PCA data\n","    features_data_train = pca.fit_transform(features_data_train)\n","    features_data_test = pca.transform(features_data_test)\n","\n","    #convert to dataframe\n","    features_data_train = pd.DataFrame(features_data_train)\n","    features_data_test = pd.DataFrame(features_data_test)\n","\n","    #drop columns to the dataset\n","    train.drop(columns=l,inplace=True)\n","    test.drop(columns=l,inplace=True)\n","\n","    #replace the columns dropped with reduced data\n","    col_name = 'PCA'+str(i)\n","    train[col_name] = features_data_train\n","    test[col_name] = features_data_test\n","    print(\"shape train after reduction -->\",train.shape)\n","    print(\"shape test after reduction -->\",test.shape)\n","    print()\n","\n","  return train, test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xv3qadlxGHlG"},"source":["'''\n","Method that plots the heatmap of correlations of features\n","passed in input.\n","@param df : the input dataset \n","@param list_of_features : the features to calculate correlation\n","'''\n","def find_correlations(df,list_of_features):\n","    ext_data = df[list_of_features]\n","    ext_data_corrs = ext_data.corr()\n","    plt.figure(figsize = (15, 10))\n","\n","    # Heatmap of correlations\n","    sns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n","    plt.title('Correlation Heatmap')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f","_cell_guid":"a5e67831-4751-4f11-8e07-527e3e092671","id":"8rhWLxNwxIuZ"},"source":["# Preprocessing and Merging\n"]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"TskvkhEnxIuw"},"source":["# read all csv\n","app_train = read_csv_and_clean('../input/home-credit-default-risk/application_train.csv')\n","app_test = read_csv_and_clean('../input/home-credit-default-risk/application_test.csv')\n","POS_CASH_balance = read_csv_and_clean(\"../input/home-credit-default-risk/POS_CASH_balance.csv\")\n","bureau = read_csv_and_clean(\"../input/home-credit-default-risk/bureau.csv\")\n","bureau_balance = read_csv_and_clean(\"../input/home-credit-default-risk/bureau_balance.csv\")\n","credit_card_balance = read_csv_and_clean(\"../input/home-credit-default-risk/credit_card_balance.csv\")\n","installments_payments = read_csv_and_clean(\"../input/home-credit-default-risk/installments_payments.csv\")\n","previous_application = read_csv_and_clean(\"../input/home-credit-default-risk/previous_application.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyWMqyP0ngUK","trusted":true},"source":["# merge application train and test with previous_application\n","app_train = merge_file_to_application(app_train,previous_application,\"PREV\",\"PREV\")\n","\n","app_test = merge_file_to_application(app_test,previous_application,\"PREV\",\"PREV\")\n","del previous_application\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHPokz2sKZeT","trusted":true},"source":["# merge bureau with bureau_balance\n","bureau_balance = bureau_balance.groupby(by=['SK_ID_BUREAU']).mean().reset_index()\n","bureau_balance.columns = change_column_name(bureau_balance.columns, \"BBALANCE\")\n","bureau = bureau.merge(bureau_balance, on =['SK_ID_BUREAU'], how = 'left')\n","bureau.update(bureau[bureau_balance.columns].fillna(0))\n","del bureau_balance\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0yhVnhpUscR","trusted":true},"source":["# merge application train and test with bureau\n","app_train = merge_file_to_application(app_train,bureau,\"BUREAU\",\"BUREAU\")\n","\n","app_test = merge_file_to_application(app_test,bureau,\"BUREAU\",\"BUREAU\")\n","del bureau\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xePo3TWApjBr","trusted":true},"source":["# merge application train and test with pos_cash_balace\n","app_train = merge_file_to_application(app_train,POS_CASH_balance,\"PREV\", \"POS\")\n","\n","app_test = merge_file_to_application(app_test,POS_CASH_balance,\"PREV\",\"POS\")\n","del POS_CASH_balance\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDPNmndvpjsY","trusted":true},"source":["# merge application train and test with credit_card_balance\n","app_train = merge_file_to_application(app_train,credit_card_balance,\"PREV\",\"CREDIT\")\n","\n","app_test = merge_file_to_application(app_test,credit_card_balance,\"PREV\",\"CREDIT\")\n","del credit_card_balance\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4RrgnfnpkFs","trusted":true},"source":["# merge application train and test with instalments_payments\n","app_train = merge_file_to_application(app_train,installments_payments ,\"PREV\", \"INSTA\")\n","\n","app_test = merge_file_to_application(app_test,installments_payments ,\"PREV\",\"INSTA\")\n","del installments_payments\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"e0d12a13cb95521c19b10d8829e8abe2b1118396","_cell_guid":"d99ca215-e893-490c-a6a4-83f3e8a067b3","trusted":true,"id":"NTKHGM53xIx3"},"source":["# align train and test data\n","app_train, app_test = align_train_test_sets(app_train,app_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d5506d0483af10dbf71e8ed11c99b2d5253680fb","_cell_guid":"bd49d18b-e35f-4122-a005-dd06d8f2f7ca","id":"nzxqXXYcxI1G"},"source":["# Feature Engineering and Selection"]},{"cell_type":"code","metadata":{"id":"pFzLn7il01n4","trusted":true},"source":["'''\n","Method that modifies some features of the application dataset\n","@dataset : the application dataset\n","@return the dataset cleaned by some features\n","'''\n","def clean_application(dataset):\n","  # drop the columns relative to FLAG DOCUMENTS\n","  flag_documents = [col for col in dataset.columns if col.startswith(\"FLAG_DOCUMENT\")]\n","  dataset = dataset.drop(columns=flag_documents)\n","  # add some domain knowledge features\n","  dataset['CREDIT_INCOME_PERCENT'] = dataset['AMT_CREDIT'] / dataset['AMT_INCOME_TOTAL']\n","  dataset['ANNUITY_INCOME_PERCENT'] = dataset['AMT_ANNUITY'] / dataset['AMT_INCOME_TOTAL']\n","  dataset['DAYS_EMPLOYED_PERCENT'] = dataset['DAYS_EMPLOYED'] / dataset['DAYS_BIRTH']\n","  dataset['INCOME_GT_CREDIT_FLAG'] = dataset['AMT_INCOME_TOTAL'] > dataset['AMT_CREDIT']\n","  dataset['CREDIT_TERM'] = dataset['AMT_CREDIT'] / dataset['AMT_ANNUITY'] \n","  dataset['INCOME_CREDIT_PERC'] = dataset['AMT_INCOME_TOTAL'] / dataset['AMT_CREDIT']\n","  dataset['INCOME_PER_PERSON'] = dataset['AMT_INCOME_TOTAL'] / dataset['CNT_FAM_MEMBERS']\n","  dataset['PAYMENT_RATE'] = dataset['AMT_ANNUITY'] / dataset['AMT_CREDIT']\n","  dataset['YEARS_BIRTH'] = dataset['DAYS_BIRTH'] / 365\n","\n","  return dataset\n","\n","app_train = clean_application(app_train)\n","app_test = clean_application(app_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlmZHHaZtUrc","trusted":true},"source":["'''\n","Method used to select a specific number of features to drop starting from the lower correlated features\n","@param correlations : a dataframe containing the correlations of features with target\n","@param features_number : the number of the feature to select and add to the drop list\n","@ return a list with features selected to drop\n","'''\n","def drop_last_correlations(correlations, features_number):\n","  correlations = correlations.reset_index()\n","  correlations['TARGET'] = correlations['TARGET'].apply(abs)\n","  correlations.sort_values(by='TARGET',inplace=True)\n","  column_dropped = correlations[:features_number]['index']\n","  return column_dropped"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhNIdTDgGwyc"},"source":["'''\n","THIS PART IS COMMENTED BECAUSE AT THE END WE USE DROP LAST CORRELATION METHOD!\n","\n","#SEARCH SIMILAR COLUMNS BY CRITERIA AND CHECK IF THEY ARE STRICTLY CORRELATED\n","cols = [x for x in app_train.columns if 'REGION' in x]\n","find_correlations(app_train,cols)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"esQlrGXqHVaG"},"source":["'''\n","THIS PART IS COMMENTED BECAUSE AT THE END WE USE DROP LAST CORRELATION METHOD!\n","\n","#Some features subset strictly correlated\n","list_of_features_AVG = ['APARTMENTS_AVG',\n"," 'BASEMENTAREA_AVG',\n"," 'YEARS_BEGINEXPLUATATION_AVG',\n"," 'YEARS_BUILD_AVG',\n"," 'COMMONAREA_AVG',\n"," 'ELEVATORS_AVG',\n"," 'ENTRANCES_AVG',\n"," 'FLOORSMAX_AVG',\n"," 'FLOORSMIN_AVG',\n"," 'LANDAREA_AVG',\n"," 'LIVINGAPARTMENTS_AVG',\n"," 'LIVINGAREA_AVG']\n"," list_of_features_MEDI = ['APARTMENTS_MEDI',\n"," 'BASEMENTAREA_MEDI',\n"," 'YEARS_BEGINEXPLUATATION_MEDI',\n"," 'YEARS_BUILD_MEDI',\n"," 'COMMONAREA_MEDI',\n"," 'ELEVATORS_MEDI',\n"," 'ENTRANCES_MEDI',\n"," 'FLOORSMAX_MEDI',\n"," 'FLOORSMIN_MEDI',\n"," 'LANDAREA_MEDI',\n"," 'LIVINGAPARTMENTS_MEDI',\n"," 'LIVINGAREA_MEDI']\n","list_of_features_AMT = ['AMT_INCOME_TOTAL',\n"," 'AMT_CREDIT',\n"," 'AMT_ANNUITY',\n"," 'AMT_GOODS_PRICE',\n"," 'AMT_REQ_CREDIT_BUREAU_HOUR',\n"," 'AMT_REQ_CREDIT_BUREAU_DAY',\n"," 'AMT_REQ_CREDIT_BUREAU_WEEK',\n"," 'AMT_REQ_CREDIT_BUREAU_MON',\n"," 'AMT_REQ_CREDIT_BUREAU_QRT',\n"," 'AMT_REQ_CREDIT_BUREAU_YEAR',\n"," 'PREV_AMT_ANNUITY',\n"," 'PREV_AMT_APPLICATION',\n"," 'PREV_AMT_CREDIT',\n"," 'PREV_AMT_DOWN_PAYMENT',\n"," 'PREV_AMT_GOODS_PRICE',\n"," 'BUREAU_AMT_CREDIT_MAX_OVERDUE',\n"," 'BUREAU_AMT_CREDIT_SUM',\n"," 'BUREAU_AMT_CREDIT_SUM_DEBT',\n"," 'BUREAU_AMT_CREDIT_SUM_LIMIT',\n"," 'BUREAU_AMT_CREDIT_SUM_OVERDUE',\n"," 'BUREAU_AMT_ANNUITY',\n"," 'CREDIT_AMT_BALANCE',\n"," 'CREDIT_AMT_CREDIT_LIMIT_ACTUAL',\n"," 'CREDIT_AMT_DRAWINGS_ATM_CURRENT',\n"," 'CREDIT_AMT_DRAWINGS_CURRENT',\n"," 'CREDIT_AMT_DRAWINGS_OTHER_CURRENT',\n"," 'CREDIT_AMT_DRAWINGS_POS_CURRENT',\n"," 'CREDIT_AMT_INST_MIN_REGULARITY',\n"," 'CREDIT_AMT_PAYMENT_CURRENT',\n"," 'CREDIT_AMT_PAYMENT_TOTAL_CURRENT',\n"," 'CREDIT_AMT_RECEIVABLE_PRINCIPAL',\n"," 'CREDIT_AMT_RECIVABLE',\n"," 'CREDIT_AMT_TOTAL_RECEIVABLE',\n"," 'INSTA_AMT_INSTALMENT',\n"," 'INSTA_AMT_PAYMENT']\n","list_of_features_PREV_DAYS = [\n"," 'DAYS_LAST_PHONE_CHANGE',\n"," 'PREV_DAYS_DECISION',\n"," 'PREV_DAYS_FIRST_DRAWING',\n"," 'PREV_DAYS_FIRST_DUE',\n"," 'PREV_DAYS_LAST_DUE_1ST_VERSION',\n"," 'PREV_DAYS_LAST_DUE',\n"," 'PREV_DAYS_TERMINATION',\n","  'INSTA_DAYS_INSTALMENT',\n"," 'INSTA_DAYS_ENTRY_PAYMENT']\n","list_of_features_PREV_ANOM =[\n"," 'PREV_DAYS_FIRST_DRAWING_ANOM',\n"," 'PREV_DAYS_FIRST_DUE_ANOM',\n"," 'PREV_DAYS_LAST_DUE_1ST_VERSION_ANOM',\n"," 'PREV_DAYS_LAST_DUE_ANOM',\n"," 'PREV_DAYS_TERMINATION_ANOM']\n","list_of_features_BUREAU = [\n"," 'BUREAU_DAYS_CREDIT',\n"," 'BUREAU_DAYS_CREDIT_ENDDATE',\n"," 'BUREAU_DAYS_ENDDATE_FACT',\n"," 'BUREAU_DAYS_CREDIT_UPDATE']\n","list_of_features_REGION = ['REGION_POPULATION_RELATIVE',\n"," 'REGION_RATING_CLIENT',\n"," 'REGION_RATING_CLIENT_W_CITY',\n"," 'REG_REGION_NOT_LIVE_REGION',\n"," 'REG_REGION_NOT_WORK_REGION',\n"," 'LIVE_REGION_NOT_WORK_REGION']\n","\n","#replace each subset with a PCA\n"," app_train,app_test = reduce_with_PCA(app_train,app_test,[list_of_features_MEDI,list_of_features_AVG,list_of_features_AMT,\n","                                                   list_of_features_PREV_DAYS,list_of_features_PREV_ANOM,list_of_features_BUREAU])\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tZYG8Y4a8MX","trusted":true},"source":["# find correlations and sort them\n","correlations = app_train.corr()['TARGET'].sort_values()\n","correlations.drop('SK_ID_CURR',inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AZnHna-YDQ6","trusted":true},"source":["#drop the selected columns\n","column_dropped = drop_last_correlations(correlations,70)\n","app_test = app_test.drop(column_dropped, axis=1)\n","app_train = app_train.drop(column_dropped, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e27d400f3ec5447cfe5e908952351f271d521784","id":"Ym8D3LKlxI2O"},"source":["# TRAINING AND TESTING"]},{"cell_type":"code","metadata":{"_uuid":"784ae2f91cf7792702595a9973ba773b2acdec00","_cell_guid":"60ef8744-ca3a-4810-8439-2835fbfc1833","trusted":true,"id":"v51JzmbjxI2V"},"source":["'''\n","Method used to prepare data to the training.\n","Convert the dataset as dataframe form in numpy array \n","useful for the models, and use some methods of cleaning data\n","like imputer scaler etc.\n","@param training_data : the dataset with train data\n","@param test_data : the dataset with test data\n","@param use_scaler : flag that indicates if use scaler during preprocessing\n","@param use_imputer : flag that indicates if use imputer during preprocessing for missing values \n","@param use_power_transform_bool : flag that indicates if use power transform on data during preprocessing and make data more Gaussian like\n","\n","@param scaler_type : type of scaler to use. Two options : 'minmax between 0-1' and 'StandardScaler'\n","@return  the training data, labels, test data as numpy array, the keys of the test clients and the features\n","'''\n","def prepare_data_for_training_and_test(training_data,\n","                                       test_data,\n","                                       use_scaler = True, \n","                                       use_imputer = True,\n","                                       use_power_transform_bool = False,\n","                                       scaler_type = \"minmax\"):\n","  \n","  #save the important columns like the keys and the targets\n","  train_labels = training_data['TARGET']\n","  train = training_data.drop(columns = ['TARGET','SK_ID_CURR'])\n","  test_keys = test_data[['SK_ID_CURR']]\n","  test = test_data.drop(columns = ['SK_ID_CURR'])\n","  features = list(train.columns)\n","\n","  imputer = SimpleImputer(strategy = 'median')\n","\n","  if use_imputer:\n","    print(\"use imputer\")\n","    imputer.fit(train)\n","    train = imputer.transform(train)\n","    test = imputer.transform(test)\n","    print(\"done\")\n","\n","  if use_scaler:\n","    print(\"use scaler\")\n","    if scaler_type == 'minmax':\n","      scaler = MinMaxScaler(feature_range = (0, 1))\n","    elif scaler_type == 'standard':\n","      scaler = StandardScaler()\n","    scaler.fit(train)\n","    train = scaler.transform(train)\n","    test = scaler.transform(test)\n","    print(\"done\")\n","\n","  if use_power_transform_bool:\n","    print(\"use power transform\")\n","    train = power_transform(train, method='yeo-johnson')\n","    test = power_transform(test, method='yeo-johnson')\n","    print(\"done\")\n","\n","  return train, train_labels, test, test_keys, features\n","\n","\n","#call the method for preprocessed data                    \n","train_p, train_labels_p, test_p,test_keys_p, features_p = prepare_data_for_training_and_test(app_train, app_test,use_scaler = True, \n","                                                                                                        use_imputer = True,\n","                                                                                                        use_power_transform_bool = False,\n","                                                                                                       scaler_type='minmax')\n","#call method for no preprocessed data\n","train, train_labels, test,test_keys, features = prepare_data_for_training_and_test(app_train, app_test,use_imputer = False,use_power_transform_bool = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2138782ddbfc9a803dc99a938460fc27d15972a9","_cell_guid":"80c77c89-3fa9-4311-b441-412a4fbb1480","trusted":true,"id":"Fb90Bc8vxI2x"},"source":["'''\n","Method used to do prediction probabilities on data passed in input using a specific\n","trained model\n","@param model : the trained model\n","@param test_data : the test data\n","@param k_id_curr_test : the client keys of the test data\n","@param file_csv : the name of the file.csv where save the submit prediction\n","@param type_pred : the type of prediction to do. Two options: 'deep' for deep learning\n","models and 'no_deep' for other models\n","'''\n","def make_prediction_and_submit(model,test_data,sk_id_curr_test,file_csv,type_pred = 'no_deep'):\n","    if(type_pred == 'deep'):\n","      predictions = model.predict_proba(test_data)\n","    elif(type_pred == 'no_deep'):\n","      predictions = model.predict_proba(test_data)[:, 1]\n","\n","    submit = sk_id_curr_test\n","    submit['TARGET'] = predictions\n","    submit.to_csv(file_csv, index = False)\n","    print(file_csv + \" created!\")\n","    \n","    del model\n","    gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YZ2yf2GZbZe","trusted":true},"source":["'''\n","Method that train and predicts the respectively training data and test data passed in input.\n","Some model use the k-fold and it is possible to choice the type of k-fold. It's possible\n","also use the SMOTE oversampling to add instances of minority class during each k-fold iteration\n","@param model_type :  it is possible to choice between 5 models using the correct string --> 3 without kfold {linear_regression,random_forest,mlp} and 2 with kfold {lgbm, xgb}\n","@param training_data : the data to train, relative to the X\n","@param training_labels : the correspective Y of the training data\n","@param test_data : the data to test\n","@param sk_id_curr_test : the keys of the clients of the test data\n","@param num_folds : if is used kfold algorithm, indicates the number of folds\n","@param file_csv : the file.csv where submit the predicted results\n","@param stratified : if selected, use the Stratified KFold, else the Normal KFold\n","@param use_over_sampling : flag that indicates if use over sampling of minority class during preprocessing\n","'''\n","def train_model(model_type,training_data,train_labels,test_data,sk_id_curr_test ,file_csv,num_folds = 5, stratified = False,use_over_sampling = False):\n","    \n","    print(\"Train with \"+model_type)\n","\n","    pred_proba = np.zeros(test_data.shape[0])\n","    \n","    #model used without KFOLD\n","    if model_type == \"linear_regression\":\n","      model = LogisticRegression(C = 0.001, random_state=42)\n","      model.fit(training_data, train_labels)\n","      pred_proba += model.predict_proba(test_data)[:, 1]\n","\n","    elif model_type == \"random_forest\":\n","      model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, \n","                                     max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,\n","                                      min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=42, verbose=1,\n","                                      warm_start=False, class_weight=None)\n","      model.fit(training_data, train_labels)\n","      pred_proba += model.predict_proba(test_data)[:, 1]\n","    \n","    elif model_type == \"mlp\":\n","      model = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant',\n","                learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=42, tol=0.0001, verbose=False, warm_start=False,\n","                momentum=0.9, nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n","      model.fit(training_data, train_labels)\n","      pred_proba += model.predict_proba(test_data)[:, 1]\n","      \n","\n","    #MODEL USED WITH KFOLD\n","    else:\n","      print(\"Train with {} Folds... \\nTrain shape: {}, test shape: {}\".format(num_folds,training_data.shape, test_data.shape))\n","      # Select type of KFOLD\n","      if stratified:\n","          kfold = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n","      else:\n","          kfold = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n","\n","      #repeat the process for each fold\n","      for n_fold, (train_idx, valid_idx) in enumerate(kfold.split(training_data, train_labels)):\n","        if(type(training_data) == pd.DataFrame):\n","          train_x, train_y = training_data.iloc[train_idx], train_labels.iloc[train_idx]\n","          valid_x, valid_y = training_data.iloc[valid_idx], train_labels.iloc[valid_idx]\n","        else:\n","          train_x, train_y = training_data[train_idx], train_labels[train_idx]\n","          valid_x, valid_y = training_data[valid_idx], train_labels[valid_idx]\n","\n","        #use SMOTE is Flag is true\n","        if use_over_sampling :\n","          print(\"oversampling\")\n","          sm = SVMSMOTE(random_state=42)\n","          train_x, train_y = sm.fit_resample(train_x, train_y)\n","          print(\"done\")\n","\n","        #select model between lgbm and xgb\n","        if model_type == \"lgbm\" :\n","          model = LGBMClassifier(\n","              nthread=4,\n","              n_estimators=200,\n","              learning_rate=0.08,\n","              num_leaves=256,\n","              colsample_bytree=0.9497036,\n","              subsample=0.8715623,\n","              max_depth=10,\n","              reg_alpha=0.041545473,\n","              reg_lambda=0.0735294,\n","              min_split_gain=0.0222415,\n","              min_child_weight=39.3259775,\n","              silent=-1,\n","              verbose=-1)\n","          \n","          model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n","            eval_metric= 'auc', verbose= 20, early_stopping_rounds= 20)\n","          pred_proba += model.predict_proba(test_data, num_iteration=model.best_iteration_)[:, 1] / kfold.n_splits\n","\n","        elif model_type == \"xgb\":\n","        ## objective = 'binary:logistic'\n","          model = XGBClassifier(learning_rate=0.2, n_estimators=200,\n","            max_depth=4,num_leaves=8, min_child_weight=39.3259775, subsample=0.87156238, colsample_by_tree=0.9497036,\n","            objective= 'binary:logistic',\n","            nthread=4)\n","          model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], early_stopping_rounds=20, eval_metric='auc',verbose=20)\n","          pred_proba += model.predict_proba(test_data,ntree_limit=model.get_booster().best_iteration)[:, 1] / kfold.n_splits\n","\n","        #Free the memory\n","        del train_x, train_y, valid_x, valid_y\n","        gc.collect()\n","\n","\n","  #submit the produced data\n","    submit = sk_id_curr_test\n","    submit['TARGET'] = pred_proba\n","    submit.to_csv(file_csv, index = False)\n","    print(file_csv + \" created\")\n","    del model\n","\n","# call the method\n","num_folds = 5\n","\n","train_model(\"lgbm\",train_p,train_labels_p,test_p,test_keys_p,\"k_fold_lgbm_p.csv\",num_folds, stratified = False)\n","train_model(\"linear_regression\",train_p,train_labels_p,test_p,test_keys_p,\"k_fold_linear_regression_p.csv\",num_folds, stratified = False)\n","train_model(\"random_forest\",train_p,train_labels_p,test_p,test_keys_p,\"k_fold_random_forest_p.csv\",num_folds, stratified = False)\n","train_model(\"mlp\",train_p,train_labels_p,test_p,test_keys_p,\"k_fold_mlp_p.csv\",num_folds, stratified = False)\n","train_model(\"xgb\",train_p,train_labels_p,test_p,test_keys_p,\"k_fold_xgb_p.csv\",num_folds, stratified = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYZ5xBq6gisa","trusted":true},"source":["# split data into train_set and eval_set\n","train_data_p, eval_data_p, train_labels_p, eval_labels_p = train_test_split(\n","    train_p, train_labels_p, test_size=0.15, random_state=None)\n","\n","train_data, eval_data, train_labels, eval_labels = train_test_split(\n","    train, train_labels, test_size=0.15, random_state=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGrM5PqEPt0O","trusted":true},"source":["#TRY TO TRAIN WITHOUT THE KFOLD\n","#PARAMETERS OF LIGHTGBM\n","#WE HAVE USED PRINCIPALLY THIS MODEL TO DO HYPERTUNING OF PARAMETERS\n","boosting_type =['gbdt','rf','dart','goss']\n","def LGBM():\n","  model_gbm = lgb.LGBMClassifier(boosting_type=boosting_type[3],\n","                                 class_weight='balanced',\n","                                 colsample_bytree=1.0, \n","                                 importance_type='split',\n","                                 learning_rate=0.09,\n","                                 max_depth=10,\n","                                 min_child_samples=60, \n","                                 min_child_weight=0.001,\n","                                 min_data=100,\n","                                 min_split_gain=0.0, \n","                                 n_estimators=500,\n","                                 n_jobs=-1,\n","                                 num_leaves=256,\n","                                 objective=None, \n","                                 random_state=42, \n","                                 silent=True, \n","                                 subsample=1.0,\n","                                 subsample_for_bin=200000, \n","                                 subsample_freq=0)\n","  return model_gbm\n","  \n","#TRAIN LGBM\n","model_gbm = LGBM()\n","print('Training with data no preprocessed')\n","model_gbm.fit(train_data, train_labels,eval_set=(eval_data,eval_labels),\n","              verbose=10,eval_metric='auc',early_stopping_rounds=20)\n","make_prediction_and_submit(model_gbm, test, test_keys, 'lightgbm_no_processed.csv')\n","\n","print()\n","\n","#train with preprocessed data\n","model_gbm = LGBM()\n","print('Training with data preprocessed')\n","model_gbm.fit(train_data_p, train_labels_p,eval_set=(eval_data_p,eval_labels_p),\n","              verbose=10,eval_metric='auc',early_stopping_rounds=20)\n","make_prediction_and_submit(model_gbm, test_p, test_keys_p, 'lightgbm_processed.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIeQSPZd0ukC","trusted":true},"source":["#TRY TO USE DEEP LEARNING WITH SEQUENTIAL KERAS MODEL AND VARIOUS HIDDEN LAYER\n","#DROPOUT IS USED TO AVOID OVERFITTING\n","\n","#define some parameters\n","input_size = train_data_p.shape[1]\n","first_units = int(input_size/2)\n","second_units = int(first_units/2)\n","\n","model = Sequential()\n","#First Hidden Layer\n","model.add(Dense(first_units, activation='relu', kernel_initializer='random_normal', input_dim=input_size))\n","model.add(Dropout(0.2))\n","#Second  Hidden Layer\n","model.add(Dense(second_units, activation='relu', kernel_initializer='random_normal'))\n","#Output Layer\n","model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n","\n","#compile the model\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()\n","\n","#parameters of training\n","batch_size = [32,64,128,256,512]\n","epochs = 20\n","early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=2, verbose=1, mode='auto', baseline=None)\n","\n","#train the model\n","history = model.fit(train_data_p, train_labels_p, epochs=epochs, batch_size=batch_size[4],validation_data=(eval_data_p,eval_labels_p),callbacks=[early_stopping])\n","\n","#predict model and submit\n","make_prediction_and_submit(model,test_p,test_keys_p,'NN_predictions.csv',type_pred='deep')"],"execution_count":null,"outputs":[]}]}
